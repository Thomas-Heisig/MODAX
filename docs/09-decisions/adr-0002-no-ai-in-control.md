# ADR-0002: KI darf nicht in Steuerung eingreifen

**Status:** Accepted  
**Date:** 2025-12-27  
**Deciders:** Thomas Heisig (Projekt-Maintainer), Safety-Architect  
**Technical Story:** Fundamentale Safety-Entscheidung f√ºr MODAX

---

## Context

MODAX integriert Machine Learning f√ºr pr√§diktive Wartung und Optimierung. Gleichzeitig m√ºssen strikte Safety-Anforderungen erf√ºllt werden. Die zentrale Frage: **Darf KI direkt in die Maschinensteuerung eingreifen?**

**Problemstellung:**
- KI/ML-Modelle sind nicht-deterministisch
- KI-Fehler k√∂nnen zu gef√§hrlichen Situationen f√ºhren
- Gleichzeitig soll KI wertvolle Insights liefern
- Wie balancieren wir Innovation und Sicherheit?

**Warum ist eine Entscheidung jetzt notwendig?**

Diese Grundsatzentscheidung beeinflusst:
- Gesamte Systemarchitektur
- API-Design
- Sicherheitskonzept
- Zertifizierungsf√§higkeit
- Benutzerakzeptanz

---

## Decision Drivers

**Sicherheit (Safety):**
- **IEC 61508 Anforderungen:** Sicherheitsfunktionen m√ºssen deterministisch sein
- **ISO 13849:** Predictable behavior erforderlich
- **Haftung:** Bei KI-Fehlern w√§re Haftung unklar
- **Zertifizierung:** KI in Safety-Funktionen erschwert Zertifizierung erheblich

**Technisch:**
- **Determinismus:** KI/ML ist inh√§rent nicht-deterministisch
- **Nachvollziehbarkeit:** "Black Box" bei komplexen Modellen
- **Fehlerrate:** Auch beste Modelle haben Fehlerquote >0%
- **Echtzeit:** KI-Inferenz kann variabel lange dauern

**Ethisch:**
- **Verantwortung:** Wer ist verantwortlich bei KI-Fehlentscheidung?
- **Transparenz:** Bediener m√ºssen verstehen, warum etwas passiert
- **Trust:** Vertrauen in automatisierte Systeme

**Praktisch:**
- **Debugging:** KI-Fehler schwer zu reproduzieren
- **Updates:** Modell-Updates k√∂nnten Safety beeinflussen
- **Wartung:** Training-Daten m√ºssen langfristig verf√ºgbar sein

---

## Considered Options

### Option 1: KI hat volle Steuerungsbefugnis

**Beschreibung:**
KI kann direkt Maschinen steuern, Parameter √§ndern, Not-Aus ausl√∂sen.

**Pros:**
- ‚úÖ Maximale Automatisierung
- ‚úÖ Schnellste Reaktion auf KI-Erkenntnisse
- ‚úÖ Keine menschliche Verz√∂gerung

**Cons:**
- ‚ùå **KRITISCH:** Nicht zertifizierbar (IEC 61508, ISO 13849)
- ‚ùå **KRITISCH:** Safety-Risiko bei KI-Fehler
- ‚ùå Unvorhersagbares Verhalten
- ‚ùå Haftungsprobleme
- ‚ùå Keine M√∂glichkeit f√ºr menschliche Intervention
- ‚ùå "Black Box" entscheidet √ºber Sicherheit

**Bewertung:**
- Performance: 5/5
- Sicherheit: **0/5 (inakzeptabel)**
- Wartbarkeit: 2/5
- Compliance: 0/5

**Fazit:** NICHT AKZEPTABEL

---

### Option 2: KI mit eingeschr√§nkten Steuerungsrechten

**Beschreibung:**
KI darf nicht-sicherheitskritische Parameter automatisch √§ndern (z.B. K√ºhlmittel), aber nicht Safety-Funktionen.

**Pros:**
- ‚úÖ Teilautomatisierung
- ‚úÖ Reaktionsf√§higkeit f√ºr unkritische Funktionen
- ‚úÖ Lerneffekte in Produktion

**Cons:**
- ‚ùå Grenze "sicherheitskritisch" vs. "unkritisch" ist flie√üend
- ‚ùå Risiko von Scope-Creep (mehr und mehr Funktionen)
- ‚ùå Schwierig zu auditieren (welche Funktionen sind OK?)
- ‚ùå Komplexe Berechtigungslogik erforderlich
- ‚ùå Teilweise Zertifizierungsprobleme

**Bewertung:**
- Performance: 4/5
- Sicherheit: 2/5 (riskant)
- Wartbarkeit: 2/5
- Compliance: 2/5

**Fazit:** Zu riskant, unklare Grenzen

---

### Option 3: KI ist rein beratend (Gew√§hlt)

**Beschreibung:**
KI analysiert Daten und gibt Empfehlungen. **Alle Entscheidungen** werden von Menschen oder deterministischer Logik getroffen.

**Pros:**
- ‚úÖ Klare Sicherheitszone (KI-frei)
- ‚úÖ Zertifizierbar (KI nicht im Safety-Scope)
- ‚úÖ Menschliche Kontrolle immer vorhanden
- ‚úÖ Auditierbar (Empfehlungen werden geloggt)
- ‚úÖ Verst√§ndlich f√ºr Bediener
- ‚úÖ Einfach zu begr√ºnden (f√ºr Audits)

**Cons:**
- ‚ùå Keine vollst√§ndige Automatisierung
- ‚ùå Verz√∂gerung durch menschliche Entscheidung
- ‚ùå Bediener k√∂nnte KI-Empfehlung ignorieren

**Bewertung:**
- Performance: 3/5 (menschliche Verz√∂gerung)
- Sicherheit: **5/5 (optimal)**
- Wartbarkeit: 5/5
- Compliance: 5/5

**Fazit:** OPTIMAL f√ºr Safety-kritisches System

---

### Option 4: Hybrid (KI mit Human-in-the-Loop f√ºr Validation)

**Beschreibung:**
KI schl√§gt vor, Mensch genehmigt oder lehnt ab, dann automatische Ausf√ºhrung.

**Pros:**
- ‚úÖ Balance zwischen Automatisierung und Kontrolle
- ‚úÖ Mensch als Safety-Funktion

**Cons:**
- ‚ùå Sehr √§hnlich zu Option 3 (faktisch gleich)
- ‚ùå "Automation Bias" (Mensch stimmt blind zu)
- ‚ùå Zus√§tzliche Komplexit√§t

**Bewertung:**
√Ñhnlich zu Option 3, aber mit Risiko von "Rubber-Stamping"

---

## Decision Outcome

**Gew√§hlte Option:** Option 3 ‚Äì KI ist rein beratend

**Begr√ºndung:**

1. **Safety First:** Sicherheit hat absolute Priorit√§t. KI in Steuerung ist inakzeptables Risiko.

2. **Zertifizierbarkeit:** IEC 61508 SIL 2 / ISO 13849 PL d sind nur erreichbar, wenn KI **nicht** Teil der Safety-Funktion ist.

3. **Nachvollziehbarkeit:** Audits erfordern klare Verantwortung. Mit KI als Berater ist klar: Mensch oder deterministische Logik entscheidet.

4. **Haftung:** Bei Unf√§llen muss nachvollziehbar sein, wer entschieden hat. KI-Empfehlung ist dokumentiert, aber Ausf√ºhrung ist menschliche/deterministische Entscheidung.

5. **Trust & Acceptance:** Bediener vertrauen einem System mehr, bei dem sie die Kontrolle behalten.

**Erwartete Konsequenzen:**

**Positive:**
- ‚úÖ Klare Audit-Trails (KI empfiehlt X, Bediener f√ºhrt Y aus)
- ‚úÖ Zertifizierung m√∂glich
- ‚úÖ Keine Safety-Incidents durch KI-Fehler
- ‚úÖ Einfach zu erkl√§ren (f√ºr Kunden, Auditoren, Versicherer)
- ‚úÖ Schutz vor "AI Hallucinations"

**Negative:**
- ‚ö†Ô∏è Langsamere Reaktion (mitigiert: KI-Empfehlungen sind nicht zeitkritisch)
- ‚ö†Ô∏è Bediener k√∂nnte gute Empfehlung ignorieren (mitigiert: Logging, Metriken)
- ‚ö†Ô∏è Keine vollautonomen Funktionen (mitigiert: Deterministische Automatisierung m√∂glich)

**Trade-offs:**
- **Automatisierung vs. Safety:** Akzeptieren wir manuelle Entscheidung f√ºr garantierte Sicherheit ‚úÖ
- **Reaktionszeit vs. Nachvollziehbarkeit:** Akzeptieren wir Verz√∂gerung f√ºr klare Verantwortung ‚úÖ

---

## Implementation

**Architektonische Durchsetzung:**

1. **Netzwerk-Trennung:**
   - KI-Ebene hat **KEINE** direkte Verbindung zu Feldebene
   - Firewall blockiert KI ‚Üí ESP32 Kommunikation

2. **MQTT-Berechtigungen:**
   ```
   # ai_layer User:
   publish: NONE  # Keine Publish-Rechte f√ºr Control Topics!
   subscribe: NONE  # Erh√§lt Daten via REST API
   ```

3. **API-Design:**
   - KI-Ebene hat nur **Read-Only** Zugriff auf Sensordaten
   - Keine Endpoints f√ºr Steuerungsbefehle in AI Layer API
   - Alle Empfehlungen werden gespeichert, nicht ausgef√ºhrt

4. **Code-Struktur:**
   ```python
   # python-ai-layer/
   # DARF NICHT importieren:
   # ‚ùå from control_layer import motion_controller
   # ‚ùå from control_layer import safety_validator
   
   # DARF NICHT haben:
   # ‚ùå mqtt_client.publish("modax/control/+/command", ...)
   ```

5. **HMI-Design:**
   - Klare visuelle Trennung:
     - **Links:** Systemstatus (Fakten)
     - **Rechts:** KI-Empfehlungen (Vorschl√§ge)
   - Button "Empfehlung anwenden" (explizite Best√§tigung)

**Prozess-Flow:**
```
1. KI analysiert Daten
2. KI erstellt Empfehlung (mit Confidence)
3. Empfehlung wird gespeichert + geloggt
4. HMI zeigt Empfehlung an
5. Bediener pr√ºft Empfehlung
6. Bediener entscheidet: Anwenden oder Ignorieren
7. Falls "Anwenden":
   - HMI sendet POST /control/command
   - Control Layer validiert (is_system_safe)
   - Falls safe: Befehl an Feldebene
8. Audit-Log: KI-Empfehlung + Bediener-Entscheidung
```

**Zeitrahmen:**
- ‚úÖ Architektur: Implementiert (v0.1.0)
- ‚úÖ MQTT-ACLs: Implementiert
- üîÑ Firewall-Regeln: Dokumentiert, optional in Deployment
- üìã Formale Sicherheits-Analyse: Geplant (v0.6.0)

---

## Validation

**Erfolgskriterien:**
- [x] KI-Ebene kann **nicht** direkt MQTT-Steuerbefehle publishen
- [x] Alle KI-Empfehlungen werden geloggt
- [x] HMI zeigt KI-Empfehlungen als "Vorschl√§ge", nicht als "Befehle"
- [x] System l√§uft weiter, wenn KI-Ebene ausf√§llt (Graceful Degradation)
- [ ] Penetration-Test: KI-Ebene versucht, Steuerung zu kompromittieren
- [ ] Audit durch externen Safety-Experten

**Metriken:**
- **KI-Empfehlungen gegeben:** [Counter]
- **KI-Empfehlungen angewendet:** [Counter]
- **KI-Empfehlungen ignoriert:** [Counter]
- **Acceptance-Rate:** Applied / Given
- **False-Positive-Rate:** (Applied + sp√§ter revertiert) / Applied

**Review-Zeitpunkt:**
- Bei jedem Major-Release (v1.0, v2.0, ...)
- Nach Safety-Incidents (falls aufgetreten)
- Bei Einf√ºhrung neuer KI-Modelle

---

## Risks & Mitigation

| Risiko | Wahrscheinlichkeit | Impact | Mitigation |
|--------|-------------------|--------|------------|
| Bediener ignoriert kritische KI-Warnung | Mittel | Hoch | UI-Design: Kritische Warnungen prominent. Eskalation. |
| "Automation Bias" (blindes Vertrauen) | Mittel | Mittel | Schulung. Confidence-Level anzeigen. Fehlerberichte. |
| Umgehung der Architektur (Bug) | Niedrig | Kritisch | Code Reviews. Automated Tests. Penetration Tests. |
| Langsame Reaktion bei echter Gefahr | Niedrig | Hoch | Hardware-Safety (ESP32) reagiert lokal ohne KI/Control. |

---

## Related Decisions

**Abh√§ngigkeiten:**
- **ADR-0001:** 4-Ebenen-Architektur (erm√∂glicht diese Trennung)

**Beeinflusst:**
- **ADR-0003:** Predictive Maintenance (KI nur empfehlend)
- **Zuk√ºnftig:** ONNX Model Deployment (ebenfalls nur empfehlend)

**Konflikte:**
- Keine

---

## Notes

**Diskussionen:**
- Option 2 (eingeschr√§nkte Rechte) wurde intensiv diskutiert, aber verworfen wegen unklarer Grenzen
- "Automation Bias" ist bekanntes Risiko, wird durch UX-Design und Schulung adressiert
- Online-Learning im Produktionsbetrieb wurde verworfen (separate Entscheidung, verst√§rkt durch diese ADR)

**Referenzen:**
- IEC 61508-1:2010 (Functional Safety of E/E/PE Safety-Related Systems)
- ISO 13849-1:2015 (Safety of Machinery ‚Äì Safety-Related Parts of Control Systems)
- "Human Factors in Automation and AI" (Parasuraman & Riley, 1997)
- "Explainable AI" (DARPA Program)

**Zitate:**
> "In safety-critical systems, AI should inform, not decide." ‚Äì Safety Engineering Principle

> "The question is not whether AI can make better decisions, but whether we can prove it always will." ‚Äì Certification Expert

**Anh√§nge:**
- [Control Boundaries](../02-system-architecture/control-boundaries.md)
- [KI-Ebene Constraints](../05-analytics-and-ml-layer/constraints.md)

---

**Autor:** Thomas Heisig  
**Reviewer:** Safety-Architect (Community Review offen)  
**Letzte Aktualisierung:** 2025-12-27

---

## Appendix: Realworld Examples

**Positive Beispiele (KI als Berater):**
- **Tesla Autopilot:** Warnt Fahrer, √ºbernimmt nicht vollst√§ndig
- **Medical AI:** Schl√§gt Diagnosen vor, Arzt entscheidet
- **Google Spam Filter:** Markiert als Spam, Nutzer kann √ºbersteuern

**Negative Beispiele (KI steuert direkt):**
- **Boeing 737 MAX MCAS:** Automatische Korrektur f√ºhrte zu Abst√ºrzen
- **Uber Self-Driving Car Fatal Crash:** Keine menschliche Kontrolle
- **Flash Crash 2010:** Algorithmen handelten autonom, B√∂rse crashte

**Lehre:** In sicherheitskritischen Bereichen ist menschliche √úberwachung essentiell.
